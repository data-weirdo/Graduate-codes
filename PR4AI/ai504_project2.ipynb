{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owh7m9N7baNm"
      },
      "source": [
        "# AI504 Project 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nLERzWfbaNm"
      },
      "source": [
        "## To-Do : Find better hyperparameters\n",
        "The goal of this project is improving the performance of Neural Machine Translation(NMT) system. In this project, you will tune the hyperparameters to achieve higher BLEU score without changing architecture and dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qnZg7YImbaNm"
      },
      "outputs": [],
      "source": [
        "from easydict import EasyDict\n",
        "import os \n",
        "# import time\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "\n",
        "# In the project, You need to change the below hyperparameters. \n",
        "config = EasyDict({\n",
        "    \"emb_dim\":256,\n",
        "    \"ffn_dim\":512,\n",
        "    \"attention_heads\":8,\n",
        "    \"dropout\":0.25,\n",
        "    \"encoder_layers\":4,\n",
        "    \"decoder_layers\":4,\n",
        "    \"lr\": 0.001,\n",
        "    \"batch_size\":1000,\n",
        "    \"nepochs\":100,\n",
        "    \"patience\":10,\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lxUcbecbaNm"
      },
      "source": [
        "## Template codes\n",
        "This code is based on the code in [Week 10](https://classum.com/main/course/16076/54). Please refer to codes & descriptions in a link for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZjjf9VJrliw"
      },
      "source": [
        "### Prelims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4Zb21bHyrliw"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade torchtext\n",
        "# !python -m spacy download de\n",
        "# !python -m spacy download en\n",
        "# !pip install -Iv --upgrade nltk==3.5\n",
        "# !pip install tensorflow --use-feature=2020-resolver\n",
        "\n",
        "\n",
        "# start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5ng6B23baNm"
      },
      "source": [
        "### Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install msgpack==0.5.6\n",
        "# !pip install tensorflow --use-feature=2020-resolver\n",
        "\n",
        "# !python -m spacy download de\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xXtGXREvbaNm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/swryu/.local/lib/python3.8/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"de\" could not be loaded, trying \"de_core_news_sm\" instead\n",
            "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n",
            "/home/swryu/.local/lib/python3.8/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "torch.manual_seed(1234)\n",
        "torch.cuda.manual_seed_all(1234)\n",
        "\n",
        "SRC = Field(tokenize = \"spacy\",\n",
        "            tokenizer_language=\"de\",\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = \"spacy\",\n",
        "            tokenizer_language=\"en\",\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
        "                                                    fields = (SRC, TRG))\n",
        "\n",
        "SRC.build_vocab(train_data, min_freq = 3)\n",
        "TRG.build_vocab(train_data, min_freq = 3)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = config.batch_size,\n",
        "    device = device,\n",
        "    shuffle=False)\n",
        "\n",
        "PAD_IDX = TRG.vocab.stoi['<pad>']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JqEbcoKbaNm"
      },
      "source": [
        "### Load model & optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dyj44zHjbaNm"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Transformer,self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(len(SRC.vocab),config.emb_dim)\n",
        "        self.decoder_embedding = nn.Embedding(len(TRG.vocab),config.emb_dim)\n",
        "        self.transformer = nn.Transformer(d_model=config.emb_dim, nhead=config.attention_heads, \n",
        "                       num_encoder_layers=config.encoder_layers, num_decoder_layers=config.decoder_layers,\n",
        "                       dim_feedforward=config.ffn_dim, dropout=config.dropout, activation='gelu')\n",
        "        self.prediction_head = nn.Linear(config.emb_dim,len(TRG.vocab))\n",
        "        \n",
        "    def forward(self, src, trg):\n",
        "        src_emb = self.encoder_embedding(src)\n",
        "        trg_emb = self.decoder_embedding(trg)\n",
        "        output = self.transformer(src_emb, trg_emb,\n",
        "                       tgt_mask=self.transformer.generate_square_subsequent_mask(trg.size(0)).to(device),\n",
        "                       src_key_padding_mask=src.eq(PAD_IDX).permute(1,0).to(device),\n",
        "                       memory_key_padding_mask=src.eq(PAD_IDX).permute(1,0).to(device),\n",
        "                       tgt_key_padding_mask=trg.eq(PAD_IDX).permute(1,0).to(device))\n",
        "        prediction = self.prediction_head(output)\n",
        "        return prediction\n",
        "\n",
        "CLIP = 1 # For gradient clipping\n",
        "    \n",
        "model = Transformer(config)\n",
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60], gamma=0.5)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIyuC73ObaNm"
      },
      "source": [
        "### Train & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip uninstall pytorch\n",
        "# # !pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y3amnpTLbaNm",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "import json\n",
        "\n",
        "prev_bleu = -1\n",
        "best_model = None\n",
        "\n",
        "def train(model: nn.Module,\n",
        "          iterator: BucketIterator,\n",
        "          optimizer: optim.Optimizer,\n",
        "          criterion: nn.Module,\n",
        "          clip: float):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for idx, batch in enumerate(iterator):\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output = output[:-1].reshape(-1, output.shape[-1])\n",
        "        trg = trg[1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module,\n",
        "             iterator: BucketIterator,\n",
        "             criterion: nn.Module):\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, batch in enumerate(iterator):\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "            output = model(src, trg)            \n",
        "            \n",
        "            output = output[:-1].reshape(-1, output.shape[-1])\n",
        "            \n",
        "            trg = trg[1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def measure_BLEU(model: nn.Module,\n",
        "             iterator: BucketIterator\n",
        "                ):\n",
        "    model.eval()\n",
        "    iterator.batch_size = 1\n",
        "    BLEU_scores = list()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(iterator):\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "            output = model(src, trg)\n",
        "            predicted = [TRG.vocab.itos[token] for token in output[:-1].argmax(dim=2).squeeze().tolist() if token!=PAD_IDX]\n",
        "            GT = [TRG.vocab.itos[token] for token in trg[1:].squeeze().tolist() if token!=PAD_IDX]\n",
        "            BLEU_scores.append(sentence_bleu([GT], predicted))\n",
        "    return sum(BLEU_scores)/len(BLEU_scores)\n",
        "\n",
        "patience=0\n",
        "\n",
        "for epoch in tqdm(range(config.nepochs), total=config.nepochs):\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    # scheduler.step()\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    test_bleu = measure_BLEU(model, test_iterator)\n",
        "    print(\"Test BLEU score : {}\".format(test_bleu * 100))\n",
        "    print(\"Epoch : {} / Training loss : {} / Validation loss : {}\".format(epoch+1, train_loss, valid_loss))\n",
        "\n",
        "    # Early stopping\n",
        "    # You can change early stop criterion\n",
        "    if prev_bleu > test_bleu:\n",
        "        patience += 1\n",
        "        if patience > config.patience:\n",
        "            break\n",
        "    else:\n",
        "        prev_bleu = test_bleu\n",
        "        patience = 0\n",
        "        best_model = deepcopy(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmpJANcerli0"
      },
      "source": [
        "## Test your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C1Jeo3u3rli0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test BLEU score : 29.65953206705337\n"
          ]
        }
      ],
      "source": [
        "# Total_time = time.time() - start_time\n",
        "# print(Total_time)\n",
        "\n",
        "test_bleu = measure_BLEU(best_model, test_iterator)\n",
        "print(\"Test BLEU score : {}\".format(test_bleu * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5WwxmvLrli0"
      },
      "source": [
        "## Save the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Lcmb8jHsrli1"
      },
      "outputs": [],
      "source": [
        "with open('config.json','w') as f:\n",
        "    json.dump(vars(config),f)\n",
        "torch.save(best_model.state_dict(),'model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1b0ZDBOrli1"
      },
      "source": [
        "## Download files\n",
        "Before execute this code, you should run the template codes first. This code will automatically downloads the state_dict of your model and configuration file which you use for training & evaluation.\n",
        "\n",
        "Please change the student ID before you run this.\n",
        "\n",
        "__CAUTION__ : Please run this code with *Google Chrome* browser. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWyUWlaorli1"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# import os\n",
        "\n",
        "# os.environ['STUDENT_ID']=\"20201234\"\n",
        "\n",
        "# if os.path.isdir('result'):\n",
        "#   !rm -rf result\n",
        "\n",
        "# %mkdir result\n",
        "# %mv config.json model.pt result\n",
        "\n",
        "# !zip $STUDENT_ID.zip result/*\n",
        "# files.download('{}.zip'.format(os.environ['STUDENT_ID']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PSgWKTnLrli1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "updating: result/ai504_project2.ipynb (deflated 80%)\n",
            "updating: result/config.json (deflated 31%)\n",
            "updating: result/model.pt (deflated 8%)\n"
          ]
        }
      ],
      "source": [
        "# ########################################################################\n",
        "# If you're using your lab server, uncomment and run the below commands #\n",
        "# ########################################################################\n",
        "\n",
        "import os\n",
        "os.environ['STUDENT_ID']=\"20201234\"\n",
        "\n",
        "if os.path.isdir('result'):\n",
        "      !rm -rf result\n",
        "\n",
        "%mkdir result\n",
        "%cp ai504_project2.ipynb result/\n",
        "%mv config.json model.pt result\n",
        "!zip $STUDENT_ID.zip result/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ai504_project2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
