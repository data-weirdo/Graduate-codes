{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AI605_final_project_baseline",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.10 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10"
    },
    "metadata": {
      "interpreter": {
        "hash": "22b7a169098165f863612545f851f4c0bd29422bf729d04a11d05250385c134c"
      }
    },
    "interpreter": {
      "hash": "22b7a169098165f863612545f851f4c0bd29422bf729d04a11d05250385c134c"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-79c5_l1JDT4"
      },
      "source": [
        "# Open-domain Question Answering (ODQA) with Dense Passage Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC46GQK_foaT"
      },
      "source": [
        "This is the DPR baseline using subset of Wikipedia, which is presented as the EfficientQA challenge baseline. \\\n",
        "- Source: https://github.com/efficientqa/efficientqa.github.io/blob/master/getting_started.md \\\n",
        "\n",
        "This baseline code loads pre-trained retriever and reader to perform ODQA. You can train your own encoders (for retrieval) and reader following the instruction in DPR repository. \n",
        "- DPR Github: https://github.com/facebookresearch/DPR\n",
        "\n",
        "[Other useful link]\n",
        "- Dense Passage Retrieval for Open-Domain Question Answering: https://arxiv.org/abs/2004.04906 \n",
        "- Natrual Questions: https://ai.google.com/research/NaturalQuestions \n",
        "- EfficientQA challenge: https://efficientqa.github.io/\n",
        "- EfficientQA baselines: https://github.com/efficientqa/retrieval-based-baselines\n",
        "- NQ open dataset: https://github.com/google-research-datasets/natural-questions/tree/master/nq_open"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtQ0vzZ4e1S5"
      },
      "source": [
        "### Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZQ7IoCcIQXT"
      },
      "source": [
        "# Fix DPR version (<1.0.0) for reproducibiltiy\n",
        "# When you train your model, you can use the latest version \n",
        "\n",
        "!git clone https://github.com/facebookresearch/DPR.git \n",
        "!cd DPR && git checkout -b under_v1 42161470d6f16d20c20f6ea2516941c224fc0b89\n",
        "!cd DPR && pip3 install .\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/DPR')\n",
        "!mkdir DPR/data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DPR' already exists and is not an empty directory.\n",
            "fatal: A branch named 'under_v1' already exists.\n",
            "Processing /home/sjyang/federated_learning/DPR\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: cython in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from dpr==0.1.0) (0.29.23)\n",
            "Requirement already satisfied: faiss-cpu>=1.6.1 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from dpr==0.1.0) (1.7.1)\n",
            "Requirement already satisfied: filelock in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from dpr==0.1.0) (3.0.12)\n",
            "Requirement already satisfied: numpy in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from dpr==0.1.0) (1.19.5)\n",
            "Requirement already satisfied: regex in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from dpr==0.1.0) (2021.4.4)\n",
            "Requirement already satisfied: torch>=1.2.0 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from dpr==0.1.0) (1.8.1+cu111)\n",
            "Requirement already satisfied: transformers<3.1.0,>=3.0.0 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from dpr==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from dpr==0.1.0) (4.49.0)\n",
            "Requirement already satisfied: wget in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from dpr==0.1.0) (3.2)\n",
            "Requirement already satisfied: spacy>=2.1.8 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from dpr==0.1.0) (3.0.6)\n",
            "Requirement already satisfied: setuptools in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from spacy>=2.1.8->dpr==0.1.0) (52.0.0.post20210125)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from spacy>=2.1.8->dpr==0.1.0) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from spacy>=2.1.8->dpr==0.1.0) (2.25.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from spacy>=2.1.8->dpr==0.1.0) (2.0.4)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from spacy>=2.1.8->dpr==0.1.0) (0.5.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from spacy>=2.1.8->dpr==0.1.0) (2.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from spacy>=2.1.8->dpr==0.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from spacy>=2.1.8->dpr==0.1.0) (8.0.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from spacy>=2.1.8->dpr==0.1.0) (0.3.2)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from spacy>=2.1.8->dpr==0.1.0) (1.7.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from spacy>=2.1.8->dpr==0.1.0) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from spacy>=2.1.8->dpr==0.1.0) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from spacy>=2.1.8->dpr==0.1.0) (3.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from spacy>=2.1.8->dpr==0.1.0) (0.7.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from spacy>=2.1.8->dpr==0.1.0) (20.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from spacy>=2.1.8->dpr==0.1.0) (3.0.5)\n",
            "Requirement already satisfied: jinja2 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from spacy>=2.1.8->dpr==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.3->spacy>=2.1.8->dpr==0.1.0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from packaging>=20.0->spacy>=2.1.8->dpr==0.1.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from pathy>=0.3.5->spacy>=2.1.8->dpr==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.8->dpr==0.1.0) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.8->dpr==0.1.0) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.8->dpr==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.8->dpr==0.1.0) (2020.12.5)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from transformers<3.1.0,>=3.0.0->dpr==0.1.0) (0.8.1rc1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from transformers<3.1.0,>=3.0.0->dpr==0.1.0) (0.1.95)\n",
            "Requirement already satisfied: sacremoses in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from transformers<3.1.0,>=3.0.0->dpr==0.1.0) (0.0.45)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from typer<0.4.0,>=0.3.0->spacy>=2.1.8->dpr==0.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from jinja2->spacy>=2.1.8->dpr==0.1.0) (2.0.1)\n",
            "Requirement already satisfied: joblib in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from sacremoses->transformers<3.1.0,>=3.0.0->dpr==0.1.0) (1.0.1)\n",
            "Requirement already satisfied: six in /home/sjyang/anaconda3/envs/nlp/lib/python3.7/site-packages (from sacremoses->transformers<3.1.0,>=3.0.0->dpr==0.1.0) (1.15.0)\n",
            "Building wheels for collected packages: dpr\n",
            "  Building wheel for dpr (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for dpr: filename=dpr-0.1.0-py3-none-any.whl size=12897 sha256=c2741fceb11c77d52a5e869fbd1de6e08a2703bcb452c0848c24a09704076e02\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-oauwrkjl/wheels/0c/3d/31/4f671b52d9268c81687ed029900a32471ece8654ee68d595d3\n",
            "Successfully built dpr\n",
            "Installing collected packages: dpr\n",
            "  Attempting uninstall: dpr\n",
            "    Found existing installation: dpr 0.1.0\n",
            "    Uninstalling dpr-0.1.0:\n",
            "      Successfully uninstalled dpr-0.1.0\n",
            "Successfully installed dpr-0.1.0\n",
            "mkdir: cannot create directory ‘DPR/data’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34bnKYBZJtNG"
      },
      "source": [
        "!pip install datasets==1.6.2\n",
        "!pip install gdown\n",
        "!pip install jsonlines"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==1.6.2\n",
            "  Downloading datasets-1.6.2-py3-none-any.whl (221 kB)\n",
            "\u001b[K     |████████████████████████████████| 221 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /home/sjyang/anaconda3/lib/python3.8/site-packages (from datasets==1.6.2) (1.2.4)\n",
            "Collecting dill\n",
            "  Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
            "Requirement already satisfied: fsspec in /home/sjyang/anaconda3/lib/python3.8/site-packages (from datasets==1.6.2) (0.9.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /home/sjyang/anaconda3/lib/python3.8/site-packages (from datasets==1.6.2) (2.25.1)\n",
            "Collecting pyarrow>=1.0.0<4.0.0\n",
            "  Downloading pyarrow-4.0.1-cp38-cp38-manylinux2014_x86_64.whl (21.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.9 MB 16.3 MB/s \n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.11.1-py38-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 60.3 MB/s \n",
            "\u001b[?25hCollecting tqdm<4.50.0,>=4.27\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 12.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /home/sjyang/anaconda3/lib/python3.8/site-packages (from datasets==1.6.2) (20.9)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp38-cp38-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 58.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/sjyang/anaconda3/lib/python3.8/site-packages (from datasets==1.6.2) (1.20.1)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading huggingface_hub-0.0.10-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions in /home/sjyang/anaconda3/lib/python3.8/site-packages (from huggingface-hub<0.1.0->datasets==1.6.2) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /home/sjyang/anaconda3/lib/python3.8/site-packages (from huggingface-hub<0.1.0->datasets==1.6.2) (3.0.12)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/sjyang/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.6.2) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sjyang/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.6.2) (1.26.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/sjyang/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.6.2) (2020.12.5)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /home/sjyang/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.6.2) (4.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /home/sjyang/anaconda3/lib/python3.8/site-packages (from packaging->datasets==1.6.2) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /home/sjyang/anaconda3/lib/python3.8/site-packages (from pandas->datasets==1.6.2) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /home/sjyang/anaconda3/lib/python3.8/site-packages (from pandas->datasets==1.6.2) (2021.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/sjyang/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.6.2) (1.15.0)\n",
            "Installing collected packages: tqdm, dill, xxhash, pyarrow, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.59.0\n",
            "    Uninstalling tqdm-4.59.0:\n",
            "      Successfully uninstalled tqdm-4.59.0\n",
            "Successfully installed datasets-1.6.2 dill-0.3.3 huggingface-hub-0.0.10 multiprocess-0.70.11.1 pyarrow-4.0.1 tqdm-4.49.0 xxhash-2.0.2\n",
            "Collecting gdown\n",
            "  Downloading gdown-3.13.0.tar.gz (9.3 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /home/sjyang/anaconda3/lib/python3.8/site-packages (from gdown) (4.49.0)\n",
            "Requirement already satisfied: filelock in /home/sjyang/anaconda3/lib/python3.8/site-packages (from gdown) (3.0.12)\n",
            "Requirement already satisfied: six in /home/sjyang/anaconda3/lib/python3.8/site-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.12.0 in /home/sjyang/anaconda3/lib/python3.8/site-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/sjyang/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /home/sjyang/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/sjyang/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown) (2020.12.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sjyang/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown) (1.26.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/sjyang/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.12.0->gdown) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for gdown: filename=gdown-3.13.0-py3-none-any.whl size=9034 sha256=32f205c79de8713002d878feaea429f54fb1cf3d444190d94a3a9782e3c8a0ef\n",
            "  Stored in directory: /home/sjyang/.cache/pip/wheels/04/51/53/ed3e97af28b242e9eb81afb4836273fbe233a14228aa82fea3\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "Successfully installed gdown-3.13.0\n",
            "Collecting jsonlines\n",
            "  Downloading jsonlines-2.0.0-py3-none-any.whl (6.3 kB)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJOCn7qZfdy7"
      },
      "source": [
        "### 1. Load Datasets\n",
        "\n",
        "Load quesetion answering datasets (NQ-open) and Wikipedia documents (for retriever).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOYS6Dud4XsU"
      },
      "source": [
        "In the final project, you will use open-domain variant of the Natural Questions datset. \\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O95rj5vzAplT"
      },
      "source": [
        "###################################\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"YOUR_GPU_NUM\"\n",
        "###################################\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "qa_dataset = load_dataset('nq_open')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset nq_open (/home/sjyang/.cache/huggingface/datasets/nq_open/nq_open/1.0.0/e2fefd08353e9ff28e75cf9849dd18e727be41e477bad044f2d7ec5200edb90c)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3lj7Asm5l2I"
      },
      "source": [
        "print(\"Num Train Samples: %d, Num Valid Samples: %d\" \n",
        "      % (len(qa_dataset['train']), len(qa_dataset['validation'])))\n",
        "qa_dataset['train'][0], qa_dataset['validation'][0]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Train Samples: 87925, Num Valid Samples: 1800\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'answer': ['Fernie Alpine Resort'],\n",
              "  'question': 'where did they film hot tub time machine'},\n",
              " {'answer': ['1988'],\n",
              "  'question': 'the last time la dodgers won the world series'})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx0A_Sxh-L7_"
      },
      "source": [
        "questions = qa_dataset['validation']['question']\n",
        "question_answers = qa_dataset['validation']['answer']\n",
        "questions[0], question_answers[0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('the last time la dodgers won the world series', ['1988'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM9vuDbYO7hD"
      },
      "source": [
        "For open-domain question answering, you need a retrieval step for finding relevant documents (or passages). To reduce the disk memory usage, this baseline uses only the subset of Wikipedia, whose documents are relevant to the question on the training data. \\\n",
        "You can find DPR performance of full vs. subset WIkipedia with disk usage in this link: https://github.com/efficientqa/efficientqa.github.io/blob/master/getting_started.md \\\n",
        "As you can see, performance of full Wikipedia is much better than the subset (EM: 41 % vs. 34.8 % for NQ-dev). You can use full Wikipedia for your final project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNdtkfMYwpk5"
      },
      "source": [
        "This code is for download subset of Wikipedia. (1GB)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmXhZE_RZR5c"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1_V-P6GEqBhr-7WoK_BpYeGxQNccEzy4Z\n",
        "!tar xf psgs_w100_subset.tar.gz -C DPR/data && rm psgs_w100_subset.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: gdown: command not found\n",
            "tar: psgs_w100_subset.tar.gz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhdFJKSuvTLN"
      },
      "source": [
        "This code is for download whole Wikipedia. (13GB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_zu24BhIC1K"
      },
      "source": [
        "# This is for whole wikipedia dump\n",
        "# !python3 /content/DPR/dpr/data/download_data.py \\\n",
        "#   --resource data.wikipedia_split --output_dir data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xli5KBC8ZmTQ"
      },
      "source": [
        "# from dense_retriever import load_passages\n",
        "\n",
        "# 제출 때 제거\n",
        "#############################################################\n",
        "import sys ###\n",
        "sys.path.insert(0, 'YOUR_DPR_DIRECTORY') ###\n",
        "############################################################\n",
        "from dense_retriever import load_passages\n",
        "\n",
        "db_path = 'DPR/data/psgs_w100_subset.tsv'\n",
        "\n",
        "all_passages = load_passages(db_path)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading data from: DPR/data/psgs_w100_subset.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYJtkCYppL-c"
      },
      "source": [
        "print(len(all_passages))\n",
        "print(all_passages['1'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1642807\n('Aaron Aaron ( or ; \"Ahärôn\") is a prophet, high priest, and the brother of Moses in the Abrahamic religions. Knowledge of Aaron, along with his brother Moses, comes exclusively from religious texts, such as the Bible and Quran. The Hebrew Bible relates that, unlike Moses, who grew up in the Egyptian royal court, Aaron and his elder sister Miriam remained with their kinsmen in the eastern border-land of Egypt (Goshen). When Moses first confronted the Egyptian king about the Israelites, Aaron served as his brother\\'s spokesman (\"prophet\") to the Pharaoh. Part of the Law (Torah) that Moses received from', 'Aaron')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiDTMAord3pm"
      },
      "source": [
        "### 2. Download and Load DPR Model \n",
        "\n",
        "Download model checkpoints and load a question encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bKLgKLid7u-"
      },
      "source": [
        "######\n",
        "# 제출할 때, options.py의 setup_args_gpu 수정 필요\n",
        "######\n",
        "import os\n",
        "import argparse\n",
        "import json\n",
        "from dpr.models import init_biencoder_components\n",
        "from dpr.utils.data_utils import Tensorizer\n",
        "from dpr.utils.model_utils import setup_for_distributed_mode, get_model_obj, load_states_from_checkpoint\n",
        "from dpr.indexer.faiss_indexers import DenseIndexer, DenseFlatIndexer\n",
        "from dense_retriever import DenseRetriever, validate, save_results\n",
        "from dpr.options import add_encoder_params, setup_args_gpu, print_args, set_encoder_params_from_state, \\\n",
        "            add_tokenizer_params, add_cuda_params, add_training_params, add_reader_preprocessing_params"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX_1l9bObqaE"
      },
      "source": [
        "Download model checkpoints (reader & retriever) and index. We will use faiss index for faster search. \\\n",
        "(faiss link: https://github.com/facebookresearch/faiss)\n",
        "\n",
        "You can also download the updated version of DPR weight for your project. Please see the DPR repository for detail. (https://github.com/facebookresearch/DPR)\n",
        "\n",
        "- Checkpoint: checkpoint.retriever.single-adv-hn.nq.bert-base-encoder\n",
        "- Wikipedia embeddings: data.retriever_results.nq.single-adv-hn.wikipedia_passages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtoiPDK7Wkqx"
      },
      "source": [
        "\n",
        "########################################################################################\n",
        "!python3 DPR/data/download_data.py --resource checkpoint.retriever.single.nq.bert-base-encoder --output_dir DPR/data # retrieval checkpoint\n",
        "\n",
        "# Subset Index\n",
        "!python3 DPR/data/download_data.py --resource indexes.single.nq.subset --output_dir DPR/data # DPR index\n",
        "\n",
        "!python3 DPR/data/download_data.py --resource checkpoint.reader.nq-single-subset.hf-bert-base --output_dir DPR/data # reader checkpoint\n",
        "########################################################################################"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading from  https://dl.fbaipublicfiles.com/dpr/checkpoint/retriever/single/nq/hf_bert_base.cp\n",
            "File already exist  DPR/data/checkpoint/retriever/single/nq/bert-base-encoder.cp\n",
            "Loading from  https://dl.fbaipublicfiles.com/dpr/checkpoint/indexes/single/nq/seen_only.index.dpr\n",
            "File already exist  DPR/data/indexes/single/nq/subset/index.dpr\n",
            "Loading from  https://dl.fbaipublicfiles.com/dpr/checkpoint/indexes/single/nq/seen_only.index_meta.dpr\n",
            "File already exist  DPR/data/indexes/single/nq/subset/index_meta.dpr\n",
            "Loading from  https://dl.fbaipublicfiles.com/dpr/checkpoint/reader/nq-single-seen_only/hf_bert_base.cp\n",
            "File already exist  DPR/data/checkpoint/reader/nq-single-subset/hf-bert-base.cp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPCVBJVNp-A7"
      },
      "source": [
        "def arguments():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # general params\n",
        "    parser.add_argument('--dpr_model_file', type=str, default=\"DOCUMENT RETRIEVAL MODEL PATH\") ###########\n",
        "    parser.add_argument('--retrieval_type', type=str, default='dpr',\n",
        "                        choices=['tfidf', 'dpr'])\n",
        "    parser.add_argument('--output_dir', type=str, default='DPR/data')\n",
        "\n",
        "  # retrieval specific params\n",
        "    parser.add_argument('--dense_index_path', type=str, default=\"DPR/data/indexes/single/nq/subset\")\n",
        "    parser.add_argument('--match', type=str, default='string', choices=['regex', 'string'])\n",
        "    parser.add_argument('--n-docs', type=int, default=100)\n",
        "    parser.add_argument('--index_buffer', type=int, default=50000,\n",
        "                        help=\"Temporal memory data buffer size (in samples) for indexer\")\n",
        "    parser.add_argument(\"--hnsw_index\", action='store_true', help='If enabled, use inference time efficient HNSW index')\n",
        "    parser.add_argument(\"--save_or_load_index\", action='store_true', default=True, help='If enabled, save index')\n",
        "\n",
        "    # reader specific params\n",
        "    add_encoder_params(parser)\n",
        "    add_training_params(parser)\n",
        "    add_tokenizer_params(parser)\n",
        "    add_reader_preprocessing_params(parser)\n",
        "\n",
        "\n",
        "    parser.add_argument(\"--max_n_answers\", default=10, type=int,\n",
        "                        help=\"Max amount of answer spans to marginalize per singe passage\")\n",
        "    parser.add_argument('--passages_per_question', type=int, default=2,\n",
        "                        help=\"Total amount of positive and negative passages per question\")\n",
        "    parser.add_argument('--passages_per_question_predict', type=int, default=40,\n",
        "                        help=\"Total amount of positive and negative passages per question for evaluation\")\n",
        "    parser.add_argument(\"--max_answer_length\", default=10, type=int,\n",
        "                        help=\"The maximum length of an answer that can be generated. This is needed because the start \"\n",
        "                             \"and end predictions are not conditioned on one another.\")\n",
        "    parser.add_argument('--eval_top_docs', type=list, default=[10, 20, 40, 50, 80, 100],\n",
        "                        help=\"top retrival passages thresholds to analyze prediction results for\")\n",
        "    parser.add_argument('--checkpoint_file_name', type=str, default='dpr_reader')\n",
        "    parser.add_argument('--prediction_results_file', type=str)\n",
        "\n",
        "\n",
        "    args = parser.parse_args(\"\")\n",
        "    args.model_file = 'DPR/data/checkpoint/reader/nq-single-subset/hf-bert-base.cp'\n",
        "    args.dev_batch_size = 8\n",
        "    args.batch_size = 8\n",
        "    args.sequence_length = 350\n",
        "    args.pretrained_model_cfg = 'bert-base-uncased'\n",
        "    args.encoder_model_type = 'hf_bert'\n",
        "    args.do_lower_case = True\n",
        "    args.prediction_results_file = 'dev_predictions.json'\n",
        "    \n",
        "\n",
        "    return args"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMk8A-Xv8uTs"
      },
      "source": [
        "args = arguments()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cQ0mhlvWrL1"
      },
      "source": [
        "saved_state = load_states_from_checkpoint(args.dpr_model_file)  \n",
        "set_encoder_params_from_state(saved_state.encoder_params, args)\n",
        "tensorizer, encoder, _ = init_biencoder_components(args.encoder_model_type, args, inference_only=True)\n",
        "encoder = encoder.question_model\n",
        "setup_args_gpu(args)\n",
        "encoder, _ = setup_for_distributed_mode(encoder, None, args.device, args.n_gpu,\n",
        "                                        args.local_rank,\n",
        "                                        args.fp16)\n",
        "encoder.eval()\n",
        "\n",
        "model_to_load = get_model_obj(encoder)\n",
        "prefix_len = len('question_model.')\n",
        "question_encoder_state = {key[prefix_len:]: value for (key, value) in saved_state.model_dict.items() if\n",
        "                          key.startswith('question_model.')}\n",
        "model_to_load.load_state_dict(question_encoder_state)\n",
        "vector_size = model_to_load.get_out_size()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading saved model from DPR/data/checkpoint/retriever/single/nq/bert-base-encoder.cp\n",
            "model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])\n",
            "Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True\n",
            "Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-uncased\n",
            "Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert\n",
            "Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/sjyang/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/sjyang/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "All model checkpoint weights were used when initializing HFBertEncoder.\n",
            "\n",
            "All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/sjyang/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/sjyang/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "All model checkpoint weights were used when initializing HFBertEncoder.\n",
            "\n",
            "All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sjyang/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "Initialized host aiamdserver01 as d.rank -1 on device=cuda, n_gpu=1, world size=1\n",
            "16-bits training: False \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5poU8lfxzJD"
      },
      "source": [
        "### 3. Retrieve relevant document (Retriever)\n",
        "\n",
        "Load retriever and indexes, and retrieve relevant documents for each question. \\\n",
        "(As this requires large memory, you can skip below cells and download the retrieved results in 5.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pqKh6UcMDeS"
      },
      "source": [
        "# load index\n",
        "# Retreival requires large memory. You can execute below cells or just load retreival result file. \n",
        "\n",
        "index_buffer_sz = args.index_buffer\n",
        "index = DenseFlatIndexer(vector_size)\n",
        "retriever = DenseRetriever(encoder, args.batch_size, tensorizer, index)\n",
        "retriever.index.deserialize_from(args.dense_index_path)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading index from DPR/data/indexes/single/nq/subset\n",
            "Loaded index of type <class 'faiss.swigfaiss.IndexFlat'> and size 1642800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw_opD3LBLMQ"
      },
      "source": [
        "questions_tensor = retriever.generate_question_vectors(questions)\n",
        "top_ids_and_scores = retriever.get_top_docs(questions_tensor.numpy(), 100) # we changed value in this. (1, 20, 40, 60, 80, 100)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoded queries 200\n",
            "Encoded queries 400\n",
            "Encoded queries 600\n",
            "Encoded queries 800\n",
            "Encoded queries 1000\n",
            "Encoded queries 1200\n",
            "Encoded queries 1400\n",
            "Encoded queries 1600\n",
            "Encoded queries 1800\n",
            "Total encoded queries tensor torch.Size([1800, 768])\n",
            "index search time: 3.196406 sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWbrgB-HxzQ-"
      },
      "source": [
        "questions_doc_hits = validate(all_passages, question_answers, top_ids_and_scores,\n",
        "                              1, args.match)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Matching answers in top docs...\n",
            "Per question validation results len=1800\n",
            "Validation results: top k documents hits [575, 750, 842, 909, 949, 980, 1002, 1020, 1037, 1054, 1069, 1085, 1096, 1100, 1107, 1115, 1120, 1124, 1132, 1138, 1148, 1152, 1162, 1165, 1170, 1173, 1178, 1181, 1183, 1186, 1194, 1199, 1204, 1207, 1208, 1215, 1217, 1220, 1226, 1227, 1230, 1231, 1235, 1238, 1239, 1243, 1246, 1249, 1250, 1250, 1251, 1251, 1252, 1254, 1254, 1257, 1258, 1258, 1260, 1260]\n",
            "Validation results: top k documents hits accuracy [0.3194444444444444, 0.4166666666666667, 0.4677777777777778, 0.505, 0.5272222222222223, 0.5444444444444444, 0.5566666666666666, 0.5666666666666667, 0.5761111111111111, 0.5855555555555556, 0.5938888888888889, 0.6027777777777777, 0.6088888888888889, 0.6111111111111112, 0.615, 0.6194444444444445, 0.6222222222222222, 0.6244444444444445, 0.6288888888888889, 0.6322222222222222, 0.6377777777777778, 0.64, 0.6455555555555555, 0.6472222222222223, 0.65, 0.6516666666666666, 0.6544444444444445, 0.6561111111111111, 0.6572222222222223, 0.6588888888888889, 0.6633333333333333, 0.6661111111111111, 0.6688888888888889, 0.6705555555555556, 0.6711111111111111, 0.675, 0.6761111111111111, 0.6777777777777778, 0.6811111111111111, 0.6816666666666666, 0.6833333333333333, 0.6838888888888889, 0.6861111111111111, 0.6877777777777778, 0.6883333333333334, 0.6905555555555556, 0.6922222222222222, 0.6938888888888889, 0.6944444444444444, 0.6944444444444444, 0.695, 0.695, 0.6955555555555556, 0.6966666666666667, 0.6966666666666667, 0.6983333333333334, 0.6988888888888889, 0.6988888888888889, 0.7, 0.7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR6bahRTg6Sh"
      },
      "source": [
        "retrieval_file = \"retrieved.json\"\n",
        "save_results(all_passages,\n",
        "            questions,\n",
        "            question_answers, #[\"\" for _ in questions],\n",
        "            top_ids_and_scores,\n",
        "            questions_doc_hits, #[[False for _ in range(args.n_docs)] for _n in questions],\n",
        "            retrieval_file)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saved results * scores  to retrieved.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDq9pVt6dMzK"
      },
      "source": [
        "len(questions_doc_hits), len(questions_doc_hits[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1800, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqI_Es9qx90X"
      },
      "source": [
        "### 4. Predict answers (Reader)\n",
        "\n",
        "Predict the final answer for the question from retrieved documents.\n",
        "Performance of this baseline (DPR-subset) is EM = 30%. \\\n",
        "You can find the performance of other baselines in this link (EfficientQa Dev)\n",
        "-  https://github.com/google-research-datasets/natural-questions/tree/master/nq_open"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LvDjHe0x-CA"
      },
      "source": [
        "# Load retrieved results\n",
        "\n",
        "from train_reader import ReaderTrainer\n",
        "\n",
        "retrieval_file = 'retrieved.json' #####\n",
        "if not os.path.exists(retrieval_file):\n",
        "  !gdown https://drive.google.com/uc?id=1_TQaJy1oBbx4BAO08SsqP8lD_65KZcqA\n",
        "\n",
        "\n",
        "setup_args_gpu(args)\n",
        "args.dev_file = retrieval_file\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initialized host aiamdserver01 as d.rank -1 on device=cuda, n_gpu=1, world size=1\n",
            "16-bits training: False \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm1VVAYHpk2d"
      },
      "source": [
        "# Predict answers and validate results\n",
        "# The prediction result file is saved as 'dev_predictions.json'\n",
        "\n",
        "class MyReaderTrainer(ReaderTrainer):\n",
        "  def _save_predictions(self, out_file, prediction_results):\n",
        "    with open(out_file, 'w', encoding=\"utf-8\") as output:\n",
        "      save_results = []\n",
        "      for r in prediction_results:\n",
        "        save_results.append({\n",
        "          'question': r.id,\n",
        "          'prediction': r.predictions[args.passages_per_question_predict].prediction_text\n",
        "          })\n",
        "        output.write(json.dumps(save_results, indent=4) + \"\\n\")\n",
        "\n",
        "trainer = MyReaderTrainer(args)\n",
        "trainer.validate()\n",
        "\n",
        "for i in range(args.num_workers):\n",
        "    os.remove(retrieval_file.replace(\".json\", \".{}.pkl\".format(i)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Initializing components for training *****\n",
            "Reading saved model from DPR/data/checkpoint/reader/nq-single-subset/hf-bert-base.cp\n",
            "model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])\n",
            "Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-uncased\n",
            "Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert\n",
            "Overriding args parameter value from checkpoint state. Param = sequence_length, value = 350\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/sjyang/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/sjyang/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "All model checkpoint weights were used when initializing HFBertEncoder.\n",
            "\n",
            "All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sjyang/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "Loading checkpoint @ batch=3940 and epoch=19\n",
            "Loading model weights from saved state ...\n",
            "Loading saved optimizer state ...\n",
            "Validation ...\n",
            "Data files: ['retrieved.json']\n",
            "Data are not preprocessed for reader training. Start pre-processing ...\n",
            "Loaded 1800 questions + retrieval results from retrieved.json\n",
            "Split data into 16 chunks\n",
            "Start batch 113\n",
            "0it [00:00, ?it/s]Start batch 113\n",
            "0it [00:00, ?it/s]Start batch 113\n",
            "0it [00:00, ?it/s]Start batch 113\n",
            "0it [00:00, ?it/s]Start batch 113\n",
            "1it [00:00,  5.35it/s]Start batch 113\n",
            "1it [00:00,  4.90it/s]Start batch 113\n",
            "1it [00:00,  5.13it/s]Start batch 113\n",
            "1it [00:00,  4.84it/s]Start batch 113\n",
            "2it [00:00,  5.38it/s]Start batch 113\n",
            "1it [00:00,  5.05it/s]Start batch 113\n",
            "1it [00:00,  5.24it/s]Start batch 113\n",
            "1it [00:00,  5.48it/s]Start batch 113\n",
            "2it [00:00,  5.18it/s]Start batch 113\n",
            "2it [00:00,  5.11it/s]Start batch 113\n",
            "2it [00:00,  5.25it/s]Start batch 105\n",
            "105it [00:19,  5.27it/s]no positive passages samples: 0\n",
            "positive passages from gold samples: 0\n",
            "105it [00:19,  5.25it/s]\n",
            "Serialize 105 results to retrieved.15.pkl\n",
            "113it [00:21,  5.35it/s]no positive passages samples: 0\n",
            "positive passages from gold samples: 0\n",
            "113it [00:21,  5.24it/s]\n",
            "Serialize 113 results to retrieved.4.pkl\n",
            "113it [00:21,  5.31it/s]no positive passages samples: 0\n",
            "positive passages from gold samples: 0\n",
            "113it [00:21,  5.20it/s]\n",
            "Serialize 113 results to retrieved.0.pkl\n",
            "113it [00:21,  5.41it/s]no positive passages samples: 0\n",
            "positive passages from gold samples: 0\n",
            "113it [00:21,  5.19it/s]\n",
            "Serialize 113 results to retrieved.1.pkl\n",
            "113it [00:21,  5.23it/s]no positive passages samples: 0\n",
            "positive passages from gold samples: 0\n",
            "113it [00:21,  5.23it/s]\n",
            "Serialize 113 results to retrieved.5.pkl\n",
            "113it [00:21,  5.23it/s]no positive passages samples: 0\n",
            "positive passages from gold samples: 0\n",
            "113it [00:21,  5.19it/s]\n",
            "Serialize 113 results to retrieved.2.pkl\n",
            "113it [00:21,  5.37it/s]no positive passages samples: 0\n",
            "positive passages from gold samples: 0\n",
            "113it [00:21,  5.20it/s]\n",
            "Serialize 113 results to retrieved.3.pkl\n",
            "113it [00:21,  5.37it/s]no positive passages samples: 0\n",
            "positive passages from gold samples: 0\n",
            "113it [00:21,  5.25it/s]\n",
            "Serialize 113 results to retrieved.8.pkl\n",
            "113it [00:21,  5.28it/s]no positive passages samples: 0\n",
            "positive passages from gold samples: 0\n",
            "113it [00:21,  5.22it/s]\n",
            "Serialize 113 results to retrieved.6.pkl\n",
            "113it [00:21,  5.41it/s]no positive passages samples: 0\n",
            "positive passages from gold samples: 0\n",
            "113it [00:21,  5.26it/s]\n",
            "Serialize 113 results to retrieved.10.pkl\n",
            "113it [00:21,  5.29it/s]no positive passages samples: 0\n",
            "positive passages from gold samples: 0\n",
            "113it [00:21,  5.22it/s]\n",
            "Serialize 113 results to retrieved.7.pkl\n",
            "113it [00:21,  5.37it/s]no positive passages samples: 0\n",
            "positive passages from gold samples: 0\n",
            "113it [00:21,  5.23it/s]\n",
            "Serialize 113 results to retrieved.9.pkl\n",
            "113it [00:21,  5.39it/s]no positive passages samples: 0\n",
            "positive passages from gold samples: 0\n",
            "113it [00:21,  5.27it/s]\n",
            "Serialize 113 results to retrieved.14.pkl\n",
            "113it [00:21,  5.27it/s]no positive passages samples: 0\n",
            "positive passages from gold samples: 0\n",
            "113it [00:21,  5.23it/s]\n",
            "Serialize 113 results to retrieved.11.pkl\n",
            "113it [00:21,  5.24it/s]no positive passages samples: 0\n",
            "positive passages from gold samples: 0\n",
            "113it [00:21,  5.25it/s]\n",
            "Serialize 113 results to retrieved.13.pkl\n",
            "113it [00:21,  5.32it/s]no positive passages samples: 0\n",
            "positive passages from gold samples: 0\n",
            "113it [00:21,  5.23it/s]\n",
            "Serialize 113 results to retrieved.12.pkl\n",
            "Chunks processed 1\n",
            "Data saved to retrieved.0.pkl\n",
            "Chunks processed 2\n",
            "Data saved to retrieved.1.pkl\n",
            "Chunks processed 3\n",
            "Data saved to retrieved.2.pkl\n",
            "Chunks processed 4\n",
            "Data saved to retrieved.3.pkl\n",
            "Chunks processed 5\n",
            "Data saved to retrieved.4.pkl\n",
            "Chunks processed 6\n",
            "Data saved to retrieved.5.pkl\n",
            "Chunks processed 7\n",
            "Data saved to retrieved.6.pkl\n",
            "Chunks processed 8\n",
            "Data saved to retrieved.7.pkl\n",
            "Chunks processed 9\n",
            "Data saved to retrieved.8.pkl\n",
            "Chunks processed 10\n",
            "Data saved to retrieved.9.pkl\n",
            "Chunks processed 11\n",
            "Data saved to retrieved.10.pkl\n",
            "Chunks processed 12\n",
            "Data saved to retrieved.11.pkl\n",
            "Chunks processed 13\n",
            "Data saved to retrieved.12.pkl\n",
            "Chunks processed 14\n",
            "Data saved to retrieved.13.pkl\n",
            "Chunks processed 15\n",
            "Data saved to retrieved.14.pkl\n",
            "Chunks processed 16\n",
            "Data saved to retrieved.15.pkl\n",
            "Preprocessed data stored in ['retrieved.0.pkl', 'retrieved.1.pkl', 'retrieved.2.pkl', 'retrieved.3.pkl', 'retrieved.4.pkl', 'retrieved.5.pkl', 'retrieved.6.pkl', 'retrieved.7.pkl', 'retrieved.8.pkl', 'retrieved.9.pkl', 'retrieved.10.pkl', 'retrieved.11.pkl', 'retrieved.12.pkl', 'retrieved.13.pkl', 'retrieved.14.pkl', 'retrieved.15.pkl']\n",
            "Reading file retrieved.0.pkl\n",
            "Aggregated data size: 113\n",
            "Reading file retrieved.1.pkl\n",
            "Aggregated data size: 226\n",
            "Reading file retrieved.2.pkl\n",
            "Aggregated data size: 339\n",
            "Reading file retrieved.3.pkl\n",
            "Aggregated data size: 452\n",
            "Reading file retrieved.4.pkl\n",
            "Aggregated data size: 565\n",
            "Reading file retrieved.5.pkl\n",
            "Aggregated data size: 678\n",
            "Reading file retrieved.6.pkl\n",
            "Aggregated data size: 791\n",
            "Reading file retrieved.7.pkl\n",
            "Aggregated data size: 904\n",
            "Reading file retrieved.8.pkl\n",
            "Aggregated data size: 1017\n",
            "Reading file retrieved.9.pkl\n",
            "Aggregated data size: 1130\n",
            "Reading file retrieved.10.pkl\n",
            "Aggregated data size: 1243\n",
            "Reading file retrieved.11.pkl\n",
            "Aggregated data size: 1356\n",
            "Reading file retrieved.12.pkl\n",
            "Aggregated data size: 1469\n",
            "Reading file retrieved.13.pkl\n",
            "Aggregated data size: 1582\n",
            "Reading file retrieved.14.pkl\n",
            "Aggregated data size: 1695\n",
            "Reading file retrieved.15.pkl\n",
            "Aggregated data size: 1800\n",
            "Total data size: 1800\n",
            "Eval step: 99 \n",
            "Eval step: 199 \n",
            "n=10\tEM 29.50\n",
            "n=20\tEM 30.33\n",
            "n=40\tEM 29.78\n",
            "n=50\tEM 29.78\n",
            "n=80\tEM 29.78\n",
            "n=100\tEM 29.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}