## AI605 Natural Language Processing  

* I thankfully got a permission to upload baseline codes from professor `Minjoon, Seo`.     
* Base codes written on Google `Colab`  
---  

### Assignment1: Text Classification with RNNs  
- Task: Make Baseline, RNN, and LSTM models (with variants) from scratch using Pytorch  
  ```  
  * Self-Reflection *
  1. Should have used 'DataLoader' to load minibatch faster (Bottleneck happened due to the absence of DataLoader)
  2. Should have made meticulous first-draft for scalability before start coding  
  3. Necessity to be familiar with Hugging Face  
  ```  

### Assignment2: Token Classification with RNNs and Attention  
- Task: Do a token classification task following some guidelines.  
  ```  
  * Self-Reflection * 
  At #4 LSTM+Attention for SQuAD part, loss was exponentially decreased comparing to previous tasks.  
  It happened after forwarding attention mechanism. I debugged but couldn't find the reason. Spent too much time at debugging.  
  ```  
  
### Assignment3: Paper writing assignment  
- No codes attached.  

### Assignment4: Sequence and Token Classification with BERT  
- Task: Sequence and Token Classification on the same tasks which were supposed on Assignment1, 2  
- Now I am quite familiar with `HuggingFace`, but what I should do from now on is as below:

  ```  
  1. Need to study low-level 'methods' in HuffingFace API.
  2. Need to concentrate on some details to improve performance. (Quite familiar with overall API, but lack details.
  3. Need to study a series of LM methodologies introduced in HuggingFace document.  
  ```  
### Final Project (with Seongjun, Yang)  
- DPR + Back-translation  
